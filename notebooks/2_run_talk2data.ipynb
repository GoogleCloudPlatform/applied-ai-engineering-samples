{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: left;\">\n",
    "    <a href=\"https://sites.google.com/corp/google.com/genai-solutions/home?authuser=0\">\n",
    "        <img src=\"https://storage.googleapis.com/miscfilespublic/Linkedin%20Banner%20%E2%80%93%202.png\" style=\"margin-right\">\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Talk2Data**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook walks through the initial embeddings setup needed for running the Talk2Data solution. \n",
    "As the solution is build on modular components, you can skip to the sections that are relevant for your use case. \n",
    "\n",
    "Currently supported Source DBs are: \n",
    "- PostgreSQL on Google Cloud SQL \n",
    "- BigQuery \n",
    "\n",
    "Furthermore, the following vector stores are supported \n",
    "- pgvector on PostgreSQL \n",
    "- BigQuery vector\n",
    "\n",
    "\n",
    "The notebook covers the following steps: \n",
    "> 1. Setting up PostgreSQL instance on Google Cloud SQL (for using both or either of PostgreSQL & pgvector)\n",
    "\n",
    "> 2. Migrating BigQuery public data to the PostgreSQL instance (for using PostgreSQL as the Source DB)\n",
    "\n",
    "> 3. Setting up BigQuery environment (for using both or either of BigQuery Source DB & Vector Store) \n",
    "\n",
    "> 4. Populating the vector store with the 'known good' question-SQL pairs (BigQuery vector DB & pgvector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“’ Using this interactive notebook\n",
    "\n",
    "Click the **run** icons â–¶ï¸  of each section within this notebook.\n",
    "\n",
    "> ðŸ’¡ Alternatively, you can run the currently selected cell with `Ctrl + Enter` (or `âŒ˜ + Enter` on a Mac).\n",
    "\n",
    "> âš ï¸ **To avoid any errors**, wait for each section to finish in their order before clicking the next â€œrunâ€ icon.\n",
    "\n",
    "This sample must be connected to a **Google Cloud project**, but nothing else is needed other than your Google Cloud project.\n",
    "\n",
    "You can use an existing project. Alternatively, you can create a new Cloud project [with cloud credits for free.](https://cloud.google.com/free/docs/gcp-free-tier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš§ **0. Getting Started**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Authenticate to Google Cloud \n",
    "Authenticate to Google Cloud as the IAM user logged into this notebook in order to access your Google Cloud Project.\n",
    "\n",
    "You can do this within Google Colab or using the Application Default Credentials in the Google Cloud CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Colab Auth\"\"\" \n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "\n",
    "\"\"\"Google CLI Auth\"\"\"\n",
    "# !gcloud auth application-default login\n",
    "\n",
    "\n",
    "import google.auth\n",
    "credentials, project_id = google.auth.default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”— Connect Your Google Cloud Project\n",
    "Time to connect your Google Cloud Project to this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'talk2data-genai-sa'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown Please fill in the value below with your GCP project ID and then run the cell.\n",
    "# PROJECT_ID = \"talk2data-genai-sa\" #@param {type:\"string\"}\n",
    "PROJECT_ID = input(\"Please enter the Google Cloud Project ID \")\n",
    "\n",
    "# Quick input validations.\n",
    "assert PROJECT_ID, \"âš ï¸ Please provide your Google Cloud Project ID\"\n",
    "\n",
    "# Configure gcloud.\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "PROJECT_ID\n",
    "# !gcloud auth application-default set-quota-project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_CLOUD_QUOTA_PROJECT']=PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_PROJECT']=PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ Set up Configuration \n",
    "The notebook will load all the configurations from the `config.ini` file in the root directory. \n",
    "If you ran the initial notebook `1_setup_envs.ipynb`, most of the parameters are already stored in the config file. \n",
    "Use the below cells to retrieve these values and specify additional ones required for this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(module_path+'/config.ini')\n",
    "\n",
    "PROJECT_ID = config['GCP']['PROJECT_ID']\n",
    "DATA_SOURCE = config['CONFIG']['DATA_SOURCE']\n",
    "VECTOR_STORE = config['CONFIG']['VECTOR_STORE']\n",
    "PG_SCHEMA = config['PGCLOUDSQL']['PG_SCHEMA']\n",
    "PG_DATABASE = config['PGCLOUDSQL']['PG_DATABASE']\n",
    "PG_USER = config['PGCLOUDSQL']['PG_USER']\n",
    "PG_REGION = config['PGCLOUDSQL']['PG_REGION'] \n",
    "PG_INSTANCE = config['PGCLOUDSQL']['PG_INSTANCE'] \n",
    "PG_PASSWORD = config['PGCLOUDSQL']['PG_PASSWORD']\n",
    "BQ_TALK2DATA_DATASET_NAME = config['BIGQUERY']['BQ_TALK2DATA_DATASET_NAME']\n",
    "BQ_LOG_TABLE_NAME = config['BIGQUERY']['BQ_LOG_TABLE_NAME'] \n",
    "BQ_DATASET_REGION = config['BIGQUERY']['BQ_DATASET_REGION']\n",
    "BQ_DATASET_NAME = config['BIGQUERY']['BQ_DATASET_NAME']\n",
    "BQ_TABLE_LIST = config['BIGQUERY']['BQ_TABLE_LIST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Please fill in the values below and then run the cell.\n",
    "# EXAMPLES = False\n",
    "EXAMPLES = input(\"Do you have examples in the csv in the nedded folder answer in True or False\").lower()\n",
    "if EXAMPLES==\"True\":\n",
    "    EXAMPLES=True\n",
    "else:\n",
    "    EXAMPLES=False\n",
    "# Quick input validations.\n",
    "# assert EXAMPLES, \"âš ï¸ Please specify whether to include the Known Good SQL pairs.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‘ **1. Create embeddings**\n",
    "The following code will fetch the table and column schema along with the known good SQL examples (the working question-SQL pairs we uploaded in the previous notebooks from the file `known_good_sql.csv`) from the specified source Database. \n",
    "\n",
    "Supported Source DBs are: \n",
    "- `cloudsql-pg`\n",
    "- `bigquery`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source selected is : bigquery\n",
      "Schema or Dataset Name is : fda_food\n"
     ]
    }
   ],
   "source": [
    "#Based on the source we need to provide Schema variable to the retrieve metadata embedding. For bigquery its dataset name and for Postgres its schema name\n",
    "from dbconnectors import pgconnector, bqconnector\n",
    "\n",
    "if DATA_SOURCE=='bigquery':\n",
    "    USER_DATABASE=BQ_DATASET_NAME \n",
    "    src_connector = bqconnector\n",
    "else: \n",
    "    USER_DATABASE=PG_SCHEMA\n",
    "    src_connector = pgconnector\n",
    "\n",
    "print(\"Source selected is : \"+ str(DATA_SOURCE) + \"\\nSchema or Dataset Name is : \"+ str(USER_DATABASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM generated 1 Table Descriptions\n",
      "\n",
      "LLM generated 7 Column Descriptions\n"
     ]
    }
   ],
   "source": [
    "from embeddings.retrieve_embeddings import retrieve_embeddings \n",
    "\n",
    "table_schema_embeddings, col_schema_embeddings, example_embeddings = retrieve_embeddings(DATA_SOURCE, EXAMPLES,SCHEMA=USER_DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_schema_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_schema_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ƒï¸ **2. Store Embeddings**\n",
    "The following code will store the retrieved embedding DataFrames in the specified Vector Store.\n",
    "\n",
    "Supported Vector Stores are: \n",
    "- `cloudsql-pgvector`\n",
    "- `bigquery-vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store selected is : cloudsql-pgvector\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector Store selected is : \"+ str(VECTOR_STORE))\n",
    "\n",
    "if VECTOR_STORE=='bigquery-vector':\n",
    "    instance_name=None\n",
    "    database_name=BQ_TALK2DATA_DATASET_NAME\n",
    "    schema=BQ_DATASET_NAME\n",
    "    database_user=None\n",
    "    database_password=None\n",
    "    region=BQ_DATASET_REGION\n",
    "    vector_connector = bqconnector\n",
    "    call_await = False\n",
    "\n",
    "else:\n",
    "    instance_name=PG_INSTANCE\n",
    "    database_name=PG_DATABASE\n",
    "    schema=PG_SCHEMA\n",
    "    database_user=PG_USER\n",
    "    database_password=PG_PASSWORD\n",
    "    region=PG_REGION\n",
    "    vector_connector = pgconnector\n",
    "    call_await=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Embeddings are stored successfully'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.store_embeddings import store_schema_embeddings\n",
    "\n",
    "await(store_schema_embeddings(table_details_embeddings=table_schema_embeddings, \n",
    "                            tablecolumn_details_embeddings=col_schema_embeddings, \n",
    "                            project_id=PROJECT_ID,\n",
    "                            instance_name=instance_name,\n",
    "                            database_name=database_name,\n",
    "                            schema=schema,\n",
    "                            database_user=database_user,\n",
    "                            database_password=database_password,\n",
    "                            region=region,\n",
    "                            EXAMPLES = EXAMPLES, \n",
    "                            VECTOR_STORE = VECTOR_STORE,\n",
    "                            example_prompt_sql_embeddings=example_embeddings \n",
    "                            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â–¶ï¸ **3. Run the Talk2Data Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mData Source :- bigquery\n",
      "Vector Store :- cloudsql-pgvector\n",
      "Schema :- fda_food\n",
      "Asked Question :- What are the top 5 cities with highest recalls?\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mData Source :- \"+ DATA_SOURCE)\n",
    "print(\"Vector Store :- \"+ VECTOR_STORE)\n",
    "print(\"Schema :- \"+USER_DATABASE)\n",
    "\n",
    "\n",
    "num_similar_matches = 10 \n",
    "similarity_score_matches = 0.3\n",
    "\n",
    "RUN_DEBUGGER = True \n",
    "EXECUTE_FINAL_SQL = True \n",
    "\n",
    "if DATA_SOURCE== \"cloudsql-pg\":\n",
    "    user_question = \"How many asian men were part of the leadership workforce in 2021?\"\n",
    "elif DATA_SOURCE == \"bigquery\":\n",
    "    user_question = \"What are the top 5 cities with highest recalls?\"\n",
    "\n",
    "prompt_for_question = \"Please enter your question for source :\" + DATA_SOURCE + \" and database : \" + USER_DATABASE\n",
    "# user_question = input(prompt_for_question) #Uncomment if you want to ask question yourself\n",
    "\n",
    "\n",
    "print(\"Asked Question :- \"+user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core.exceptions import NotFound\n",
    "from google.cloud import aiplatform\n",
    "import pandas as pd\n",
    "import vertexai\n",
    "\n",
    "from agents import EmbedderAgent, BuildSQLAgent, DebugSQLAgent, ValidateSQLAgent, ResponseAgent\n",
    "\n",
    "\n",
    "embedder = EmbedderAgent('vertex') \n",
    "\n",
    "SQLBuilder = BuildSQLAgent('gemini-pro')\n",
    "SQLChecker = ValidateSQLAgent('gemini-pro')\n",
    "SQLDebugger = DebugSQLAgent()\n",
    "Responder = ResponseAgent('gemini-pro')\n",
    "\n",
    "AUDIT_TEXT=''\n",
    "found_in_vector = 'N'\n",
    "process_step=''\n",
    "final_sql='Not Generated Yet'\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=region)\n",
    "aiplatform.init(project=PROJECT_ID, location=region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No exact match found for the user prompt\n",
      "Did not find any results. Adjust the query parameters.\n",
      "\n",
      " SQL Syntax Validity:True\n",
      "\n",
      " SQL Syntax Error Description:[]\n",
      "\n",
      "This query will process 612376 bytes.\n",
      "exec_result_df:This query will process 612376 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Fetch the embedding of the user's input question \n",
    "embedded_question = embedder.create(user_question)\n",
    "AUDIT_TEXT = AUDIT_TEXT + \"User Question : \" + str(user_question) + \"\\nUser Database : \" + str(USER_DATABASE)\n",
    "process_step = \"Get Exact Match\"\n",
    "# Look for exact matches in known questions \n",
    "exact_sql_history = vector_connector.getExactMatches(user_question) \n",
    "\n",
    "if exact_sql_history is not None:\n",
    "    found_in_vector = 'Y' \n",
    "    final_sql = exact_sql_history\n",
    "    invalid_response = False\n",
    "    AUDIT_TEXT = AUDIT_TEXT + \"\\nExact match has been found\" \n",
    "\n",
    "\n",
    "else:\n",
    "    # No exact match found. Proceed looking for similar entries in db \n",
    "    AUDIT_TEXT = AUDIT_TEXT + \"\\nNo exact match found looking for similar entries and schema\"\n",
    "    process_step = \"Get Similar Match\"\n",
    "    if call_await:\n",
    "        similar_sql = await vector_connector.getSimilarMatches('example', USER_DATABASE, embedded_question, num_similar_matches, similarity_score_matches)\n",
    "    else:\n",
    "        similar_sql = vector_connector.getSimilarMatches('example', USER_DATABASE, embedded_question, num_similar_matches, similarity_score_matches)\n",
    "\n",
    "    process_step = \"Get Table and Column Schema\"\n",
    "    # Retrieve matching tables and columns\n",
    "    if call_await: \n",
    "        tables_schema =  await vector_connector.getSimilarMatches('table', USER_DATABASE, embedded_question, num_similar_matches, similarity_score_matches)\n",
    "        tables_detailed_schema =  await vector_connector.getSimilarMatches('column', USER_DATABASE, embedded_question, num_similar_matches, similarity_score_matches)\n",
    "    else:\n",
    "        tables_schema =  vector_connector.getSimilarMatches('table', USER_DATABASE, embedded_question, num_similar_matches, similarity_score_matches)\n",
    "        tables_detailed_schema =  vector_connector.getSimilarMatches('column', USER_DATABASE, embedded_question, num_similar_matches, similarity_score_matches)\n",
    "\n",
    "    AUDIT_TEXT = AUDIT_TEXT + \"\\n Retrived Similar Entries, Table Schema and Column Schema\"\n",
    "    # If similar table and column schemas found: \n",
    "    if len(tables_schema.replace('Schema(values):','').replace(' ','')) > 0 or len(tables_detailed_schema.replace('Column name(type):','').replace(' ','')) > 0 :\n",
    "\n",
    "        # GENERATE SQL\n",
    "        process_step = \"Build SQL\"\n",
    "        generated_sql = SQLBuilder.build_sql(DATA_SOURCE,user_question,tables_schema,tables_detailed_schema,similar_sql)\n",
    "        final_sql=generated_sql\n",
    "        AUDIT_TEXT = AUDIT_TEXT + \"\\n Generated SQL : \" + str(generated_sql)\n",
    "        \n",
    "        if 'unrelated_answer' in generated_sql :\n",
    "            invalid_response=True\n",
    "\n",
    "        # If agent assessment is valid, proceed with checks  \n",
    "        else:\n",
    "            invalid_response=False\n",
    "\n",
    "            if RUN_DEBUGGER: \n",
    "                generated_sql, invalid_response,AUDIT_TEXT = SQLDebugger.start_debugger(DATA_SOURCE,generated_sql, user_question, SQLChecker, tables_schema, tables_detailed_schema, similar_sql) \n",
    "\n",
    "            final_sql=generated_sql\n",
    "            AUDIT_TEXT = AUDIT_TEXT + \"\\nFinal SQL after Debugger : \" +str(final_sql)\n",
    "\n",
    "\n",
    "    # No matching table found \n",
    "    else:\n",
    "        invalid_response=True\n",
    "        print('No tables found in Vector ...')\n",
    "        AUDIT_TEXT = AUDIT_TEXT + \"\\n No tables have been found in the Vector DB...\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Question: What are the top 5 cities with highest recalls?\n",
      "\n",
      "\n",
      " Final SQL Execution Result: \n",
      "\n",
      "Final Answer :The top 5 cities with the highest number of recalls are:\n",
      "\n",
      "1. Chicago with 870 recalls\n",
      "2. Milwaukee with 637 recalls\n",
      "3. Baltimore with 443 recalls\n",
      "4. Austin with 382 recalls\n",
      "5. Troy with 370 recalls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Log Row added'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not invalid_response:\n",
    "    try: \n",
    "        if EXECUTE_FINAL_SQL is True:\n",
    "                final_exec_result_df=src_connector.retrieve_df(final_sql.replace(\"```sql\",\"\").replace(\"```\",\"\").replace(\"EXPLAIN ANALYZE \",\"\"))\n",
    "                print('\\n Question: ' + user_question + '\\n')\n",
    "                # print('\\n Final SQL Execution Result: \\n')\n",
    "                # print(final_exec_result_df)\n",
    "                response = final_exec_result_df\n",
    "                _resp=Responder.run(user_question, response)\n",
    "                AUDIT_TEXT = AUDIT_TEXT + \"\\n Model says \" + str(_resp) \n",
    "\n",
    "\n",
    "        else:  # Do not execute final SQL\n",
    "                print(\"Not executing final SQL since EXECUTE_FINAL_SQL variable is False\\n \")\n",
    "                response = \"Please enable the Execution of the final SQL so I can provide an answer\"\n",
    "                _resp=Responder.run(user_question, response)\n",
    "                AUDIT_TEXT = AUDIT_TEXT + \"\\n Model says \" + str(_resp) \n",
    "\n",
    "    except ValueError: \n",
    "          print('')\n",
    "    # except Exception as e: \n",
    "    #     print(f\"An error occured. Aborting... Error Message: {e}\")\n",
    "        \n",
    "else:  # Do not execute final SQL\n",
    "    print(\"Not executing final SQL as it is invalid, please debug!\")\n",
    "    response = \"I am sorry, I could not come up with a valid SQL.\"\n",
    "    _resp=Responder.run(user_question, response)\n",
    "    AUDIT_TEXT = AUDIT_TEXT + \"\\n Model says \" + str(_resp)\n",
    "\n",
    "print(\"Final Answer :\" + str(_resp))\n",
    "bqconnector.make_audit_entry(DATA_SOURCE, USER_DATABASE, \"gemini-pro\", user_question, final_sql, found_in_vector, \"\", process_step, \"\", AUDIT_TEXT)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—‘ Clean Up Notebook Resources\n",
    "Make sure to delete your Cloud SQL instance and BigQuery Dataset when your are finished with this notebook to avoid further costs. ðŸ’¸ ðŸ’°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete Cloud SQL instance\n",
    "!gcloud sql instances delete {PG_INSTANCE} -q\n",
    "\n",
    "#delete BigQuery Dataset using bq utility\n",
    "!bq rm -r -f -d {BQ_DATASET_NAME}\n",
    "\n",
    "#delete BigQuery Talk2Data Dataset Processing Dataset using bq utility\n",
    "\n",
    "!bq rm -r -f -d {BQ_TALK2DATA_DATASET_NAME}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
