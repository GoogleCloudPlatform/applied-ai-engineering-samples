{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX2wUzd3gjTc"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk0wepCDBa-d"
      },
      "source": [
        "# Using Gemini Long Context Window for Video\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/gemini/prompting_recipes/long_context_window/gemini_long_context_video.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fmain%2Fgenai-on-vertex-ai%2Fgemini%2Fprompting_recipes%2Flong_context_window%2Fgemini_long_context_video.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/applied-ai-engineering-samples/main/genai-on-vertex-ai/gemini/prompting_recipes/long_context_window/gemini_long_context_video.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/gemini/prompting_recipes/long_context_window/gemini_long_context_video.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2222be4842e7"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Vijay Reddy](https://github.com/vijaykyr) |\n",
        "| Reviewer(s) | [Rajesh Thallam](https://github.com/rthallam), [Skander Hannachi](https://github.com/skanderhn)  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b53Y2SxXdjAR"
      },
      "source": [
        "# Overview\n",
        "\n",
        "---\n",
        "\n",
        "Gemini 1.5 Pro supports up to 2 Million input tokens. This is the equivalent of roughly:\n",
        "- ~2000 pages of text\n",
        "- ~19 hours of audio\n",
        "- ~2 hours of video\n",
        "- ~60K lines of code\n",
        "\n",
        "This [long context window](https://cloud.google.com/vertex-ai/generative-ai/docs/long-context) (LCW) opens up possibilities for new use cases and optimizing standard use cases such as:\n",
        "- Analyzing video(s) and identifying key moments\n",
        "- Incident analysis in videos to identify policy violations\n",
        "- Transcribing, summarizing conversations such as podcasts\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook we will demonstrate Gemini's capability of understanding long context window (LCW) using the [video modality](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding)*.\n",
        "\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "* For example of text modality see the companion <a href=\"https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/gemini/long_context_window/gemini_long_context_text.ipynb\">text notebook</a>.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOy2p5tQS5PV"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "The following steps are necessary to run this notebook, no matter what notebook environment you're using.\n",
        "\n",
        "If you're entirely new to Google Cloud, [get started here](https://cloud.google.com/docs/get-started)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "546f03289832"
      },
      "source": [
        "## Google Cloud Project Setup\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "1. [Enable the Service Usage API](https://console.cloud.google.com/apis/library/serviceusage.googleapis.com)\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "1. [Enable the Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66caf6693c7f"
      },
      "source": [
        "## Google Cloud Permissions\n",
        "\n",
        "**To run the complete Notebook, including the optional section, you will need to have the [Owner role](https://cloud.google.com/iam/docs/understanding-roles) for your project.**\n",
        "\n",
        "If you want to skip the optional section, you need at least the following [roles](https://cloud.google.com/iam/docs/granting-changing-revoking-access):\n",
        "* **`roles/serviceusage.serviceUsageAdmin`** to enable APIs\n",
        "* **`roles/iam.serviceAccountAdmin`** to modify service agent permissions\n",
        "* **`roles/aiplatform.user`** to use AI Platform components\n",
        "* **`roles/storage.objectAdmin`** to modify and delete GCS buckets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxjiHKjmS90b"
      },
      "source": [
        "## Install Vertex AI SDK for Python and other dependencies (If Needed)\n",
        "\n",
        "The list `packages` contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o3hnMgWTDh3"
      },
      "outputs": [],
      "source": [
        "! pip install google-cloud-aiplatform --upgrade --quiet --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd4wqoojTd1Q"
      },
      "source": [
        "## Restart Runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqTCKVIrTgMD"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlTYFeIAbaSZ"
      },
      "source": [
        "## Authenticate\n",
        "\n",
        "If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud [project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects).\n",
        "\n",
        "If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into [Application Default Credentials for your local environment](https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev) and [initializing the Google Cloud CLI](https://cloud.google.com/docs/authentication/gcloud). In many cases, running `gcloud auth application-default login` in a shell on the machine running the notebook kernel is sufficient.\n",
        "\n",
        "More authentication options are discussed [here](https://cloud.google.com/docs/authentication)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-wkI7tebdOd"
      },
      "outputs": [],
      "source": [
        "# Colab authentication.\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    print(\"Authenticated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JUY_QXRcqLJ"
      },
      "source": [
        "## Set Google Cloud project information and Initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "Make sure to change `PROJECT_ID` in the next cell. You can leave the values for `REGION` unless you have a specific reason to change them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voBgOrpgcnPD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vertex AI SDK initialized.\n",
            "Vertex AI SDK version = 1.64.0\n"
          ]
        }
      ],
      "source": [
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "PROJECT_ID = \"rthallam-demo-project\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "print(\"Vertex AI SDK initialized.\")\n",
        "print(f\"Vertex AI SDK version = {vertexai.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3T9vgcFH1Cg"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e69abc8cbedc"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "from IPython.display import Markdown\n",
        "from vertexai.preview import caching\n",
        "from vertexai.preview.generative_models import (GenerativeModel,\n",
        "                                                HarmBlockThreshold,\n",
        "                                                HarmCategory, Part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f03757456595"
      },
      "source": [
        " ## Initialize Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVAqvXSiHv6L"
      },
      "outputs": [],
      "source": [
        "# Gemini Config\n",
        "GENERATION_CONFIG = dict(temperature=0, seed=1)\n",
        "\n",
        "SAFETY_CONFIG = {\n",
        "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcXoABv0ibFJ"
      },
      "source": [
        "# Long Context for Video Analysis\n",
        "\n",
        "To demonstrate Gemini's long context capabilities in the video modality we will use two videos from I/O 2024, Google's annual developer conference. \n",
        "\n",
        "1. The [opening keynote](https://youtu.be/uFroTufv6es). It is 21 minutes long and ~370K tokens. \n",
        "2. The [deepmind Keynote](https://youtu.be/NVwUMyYuLtw). It is 17 minutes and ~300K tokens.\n",
        "\n",
        "We will start with some questions single video questions, then we will demonstrate multi-video prompting by including both videos as context for a total of ~670K tokens.\n",
        "\n",
        "These videos are publically available on youtube, however since the Gemini API requires video content to be staged in Google Cloud Storage we store copies of these videos there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f01883155896"
      },
      "outputs": [],
      "source": [
        "OPENING_URI = \"gs://gen-ai-assets-public/Google_IO_2024_Keynote_Opening.mp4\"\n",
        "DEEPMIND_URI = \"gs://gen-ai-assets-public/Google_IO_2024_Keynote_Deepmind.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da81022b3abe"
      },
      "source": [
        "## Single Video Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8b217cdd226"
      },
      "source": [
        "### Caching context for repeated long context prompts\n",
        "\n",
        "For any repeated long context prompts it is best practice to first cache. Caching large inputs improves  cost significantly by avoiding reprocessing large input in every request. For more detailed analysis on the cost savings of caching see [this notebook](https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/gemini/long_context_window/gemini_long_context_text.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e731bcf84d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 37.2 ms, sys: 11.7 ms, total: 48.9 ms\n",
            "Wall time: 20.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "system_instruction = \"\"\"\n",
        "Here is the opening keynote from Google I/O 2024. Based on the video answer the following questions.\n",
        "\"\"\"\n",
        "\n",
        "contents = [\n",
        "    Part.from_uri(OPENING_URI, mime_type=\"video/mp4\"),\n",
        "]\n",
        "\n",
        "# create cache\n",
        "cached_content = caching.CachedContent.create(\n",
        "    model_name=\"gemini-1.5-pro-001\",\n",
        "    system_instruction=system_instruction,\n",
        "    contents=contents,\n",
        "    ttl=datetime.timedelta(minutes=30),\n",
        ")\n",
        "cached_content = caching.CachedContent(cached_content_name=cached_content.name)\n",
        "\n",
        "# configure model to read from cache\n",
        "model_cached = GenerativeModel.from_cached_content(\n",
        "    cached_content=cached_content,\n",
        "    generation_config=GENERATION_CONFIG,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd70fe8c6c84"
      },
      "source": [
        "### Prompt #1: Video analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6f7845e2ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 55.6 ms, sys: 1.5 ms, total: 57.1 ms\n",
            "Wall time: 1min 13s\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "The video takes place at the Google I/O 2024 keynote, held at the Shoreline Amphitheatre in Mountain View, California. The CEO of Google, Sundar Pichai, is giving the opening keynote speech. The stage has a large screen displaying the Google logo and various presentations. The audience consists of thousands of developers, with millions more joining virtually around the world. \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "response = model_cached.generate_content(\n",
        "    \"Describe the setting in which the video takes place\"\n",
        ")\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7b10d7f658"
      },
      "source": [
        "#### Analysis\n",
        "\n",
        "This response demonstrates Gemini's use of both audio and visual signals in the video.\n",
        "- *'The stage has a large screen behind it, and there is a podium with two laptops on it.'*. This is a purely visual cue.\n",
        "- *'The audience consists thousands of developers, with millions more joining virtually around the world.'* This is an audio cue as the speaker says this. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f83764f8686c"
      },
      "source": [
        "### Prompt #2: Key event detection from video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96086ad7c30b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 66 ms, sys: 15.2 ms, total: 81.2 ms\n",
            "Wall time: 1min 33s\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Sure, here are the timestamps of all the applauses in the video:\n",
              "\n",
              "- 01:31-01:47\n",
              "- 05:45-05:51\n",
              "- 06:50-06:54\n",
              "- 07:43-07:49\n",
              "- 11:04-11:11\n",
              "- 11:35-11:41\n",
              "- 12:08-12:15\n",
              "- 16:53-16:58\n",
              "\n",
              "Let me know if you have any other questions. \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "response = model_cached.generate_content(\n",
        "    \"Give me the timestamps of all applauses in the video with a start and end time (MM:SS).\"\n",
        ")\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23e4966efd5"
      },
      "source": [
        "#### Analysis\n",
        "\n",
        "This response demonstrates Gemini's retrieval accuracy over the span of the video, and could be used streamline editing a video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1308ac2d6e82"
      },
      "source": [
        "### Prompt #3: Focus on visual content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "276dc703566d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 34.7 ms, sys: 1.94 ms, total: 36.7 ms\n",
            "Wall time: 31.4 s\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "The speaker most frequently uses a gesture where he brings his hands together in front of his chest, with his palms facing each other and fingers loosely interlocked. He often moves his hands slightly apart and back together while speaking. \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "response = model_cached.generate_content(\n",
        "    \"Describe the hand gesture the speaker uses most frequently.\"\n",
        ")\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70e88b18583a"
      },
      "source": [
        "#### Analysis\n",
        "\n",
        "This response illustrates Gemini's attention to subtle visual details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccf01d5d0291"
      },
      "source": [
        "### Prompt #4: Attention to text and visual details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24bba1677c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 32.9 ms, sys: 28.8 ms, total: 61.8 ms\n",
            "Wall time: 37.8 s\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "The live demo was presented by Josh Woodward. "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "response = model_cached.generate_content(\"Who presented the live demo?\")\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5acab8202c0a"
      },
      "source": [
        "#### Analysis\n",
        "\n",
        "In the video Josh is only introduced by his first name, while his full name is briefly shown on a slide. Gemini is able to pick up on this text and associate it with the name of the speaker. It is also able to differentiate the demo portion of the talk from the main speaker (Sundar Pichai). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45eddf65b5ea"
      },
      "source": [
        "## Multi Video Prompts\n",
        "\n",
        "Now let's include multiple videos in the prompt. Gemini 1.5 Pro model currently supports up to 10 videos per prompt with total video length of ~2hrs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d3e3fc30d21"
      },
      "source": [
        "### Caching videos in the long context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3155a16db7b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 91.7 ms, sys: 14.1 ms, total: 106 ms\n",
            "Wall time: 54.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "system_instruction = \"\"\"\n",
        "Here are two videos from Google I/O 2024. \n",
        "The first is the opening keynote and the second is the Google DeepMind keynote. \n",
        "\"\"\"\n",
        "\n",
        "contents = [\n",
        "    Part.from_uri(OPENING_URI, mime_type=\"video/mp4\"),\n",
        "    Part.from_uri(DEEPMIND_URI, mime_type=\"video/mp4\"),\n",
        "    \"Based on the videos answer the following questions.\",\n",
        "]\n",
        "\n",
        "# create cache\n",
        "cached_content = caching.CachedContent.create(\n",
        "    model_name=\"gemini-1.5-pro-001\",\n",
        "    system_instruction=system_instruction,\n",
        "    contents=contents,\n",
        "    ttl=datetime.timedelta(minutes=30),\n",
        ")\n",
        "cached_content = caching.CachedContent(cached_content_name=cached_content.name)\n",
        "\n",
        "# configure model to read from cache\n",
        "model_cached = GenerativeModel.from_cached_content(\n",
        "    cached_content=cached_content,\n",
        "    generation_config=GENERATION_CONFIG,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71fb96961f38"
      },
      "source": [
        "### Prompt #5: Analyzing and comparing two videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecd9eae8b19b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 67.1 ms, sys: 23.2 ms, total: 90.2 ms\n",
            "Wall time: 54 s\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "The first video is the Google I/O 2024 opening keynote, presented by Sundar Pichai. It focuses on the advancements in AI, particularly the Gemini model, and its integration into various Google products like Search, Photos, and Workspace. The video highlights the capabilities of Gemini, including its multimodal reasoning, long context window, and ability to handle complex queries. It also showcases the potential of AI agents in simplifying everyday tasks.\n",
              "\n",
              "The second video is the Google DeepMind keynote, presented by Demis Hassabis. It delves deeper into the research and development behind Gemini, emphasizing its foundation in neuroscience and the goal of achieving artificial general intelligence (AGI). The video showcases specific examples of DeepMind's work, including AlphaFold 3 for protein structure prediction, Project Astra for AI agents, Imagen 3 for image generation, and Veo for generative video.\n",
              "\n",
              "Here's a table summarizing the key differences:\n",
              "\n",
              "| Feature | Google I/O Keynote | Google DeepMind Keynote |\n",
              "|---|---|---|\n",
              "| **Focus** | Gemini's integration into Google products and its impact on users | DeepMind's research and development efforts in AI, particularly Gemini |\n",
              "| **Speaker** | Sundar Pichai | Demis Hassabis |\n",
              "| **Key Highlights** | Gemini's capabilities, AI agents, user-focused applications | Technical advancements, AGI, specific projects like AlphaFold, Astra, Imagen, and Veo |\n",
              "| **Target Audience** | General audience, developers, users | Researchers, developers, AI enthusiasts |\n",
              "\n",
              "In essence, the Google I/O keynote provides a broader overview of Gemini and its applications, while the DeepMind keynote offers a more technical and research-oriented perspective.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "res = model_cached.generate_content(\"How do the videos differ?\")\n",
        "Markdown(res.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "431aa4a534c8"
      },
      "source": [
        "##### **Analysis**\n",
        "\n",
        "This response demonstrates comparative analysis of two videos. It requires first an understanding of the contents of each individual video, then being able to reason about how they differ. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e628595480a9"
      },
      "source": [
        "### Prompt #6: Information retrieval across videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c973fc8cdc8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 50.3 ms, sys: 41.2 ms, total: 91.5 ms\n",
            "Wall time: 1min 16s\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Sure, here are the new features launched based on the video provided:\n",
              "\n",
              "* **AI Overviews** - A new search experience that allows users to ask longer and more complex questions, even searching with photos.\n",
              "* **Ask Photos** - A new feature in Google Photos that allows users to search their memories in a deeper way by asking questions about their photos.\n",
              "* **2 Million Tokens Context Window** - An expansion of the context window in Gemini 1.5 Pro to 2 million tokens, opening up new possibilities for developers.\n",
              "* **Audio Overviews** - A new feature in NotebookLM that allows users to listen to a lively science discussion personalized for them based on the text material they provide.\n",
              "* **Gemini 1.5 Flash** - A lighter-weight model compared to Gemini 1.5 Pro, designed to be fast and cost-efficient to serve at scale while still featuring multimodal reasoning capabilities and breakthrough long context.\n",
              "* **Project Astra** - A universal AI agent that can be truly helpful in everyday life.\n",
              "* **Imagen 3** - Google's most capable image generation model yet, featuring stronger evaluations, extensive red teaming, and state-of-the-art watermarking with SynthID.\n",
              "* **Music AI Sandbox** - A suite of professional music AI tools that can create new instrumental sections from scratch, transfer styles between tracks, and more.\n",
              "* **Veo** - Google's newest and most capable generative video model, capable of creating high-quality 1080p videos from text, image, and video prompts.\n",
              "\n",
              "Please note that some of these features are still in development and may not be available to the public yet.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "res = model_cached.generate_content(\n",
        "    \"What new features were launched? Format your response as a bulleted list.\"\n",
        ")\n",
        "Markdown(res.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c985ff1abce"
      },
      "source": [
        "#### Analysis\n",
        "\n",
        "This response illustrates retrieval across multiple videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f72cbe2fe775"
      },
      "source": [
        "### Prompt #7: Targeted video analysis and relevant detail extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe3b52603cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 40 ms, sys: 32.9 ms, total: 72.9 ms\n",
            "Wall time: 46.6 s\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "The video shows two technologies that can help artists:\n",
              "\n",
              "1. **Music AI Sandbox:** This is a suite of professional music AI tools that can create new instrumental sections from scratch, transfer styles between tracks, and more.\n",
              "2. **Veo:** This is a generative video model that can create high-quality 1080p videos from text, image, and video prompts. It can capture the details of your instructions in different visual and cinematic styles. You can prompt for things like aerial shots of a landscape or a timelapse and further edit your videos using additional prompts.\n",
              "\n",
              "Both of these technologies are powered by Google's Gemini AI model.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "res = model_cached.generate_content(\n",
        "    \"What technologies were introduced that can help artists?\"\n",
        ")\n",
        "Markdown(res.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f120b4f94ff"
      },
      "source": [
        "#### Analysis\n",
        "\n",
        "The artist collaborations are shown in the second video only. Gemini is able to isolate this video and pick out the relevant technologies mentioned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ2KpkZ0hLm0"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf81638aa73c"
      },
      "source": [
        "The notebook demonstrated combining Gemini's long context and multimodal capability to analyze videos of considerable length. Gemini has demonstrated competence on retrieval, description, and reasoning tasks on both single and multi video prompts."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gemini_long_context_video.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
